{
    "item_type": "proposal",
    "title": "Performance Capture of the Real World in Motion",
    "abstract": "Computer graphics technology for realistic rendering has improved dramatically; however, the technology to create scene models to be rendered, e.g., for movies, has not developed at the same pace. In practice, the state of the art in model creation still requires months of complex manual design, and this is a serious threat to progress. To attack this problem, computer graphics and computer vision researchers jointly developed methods that capture scene models from real world examples. Of particular importance is the capturing of moving scenes. The pinnacle of dynamic scene capture technology in research is marker-less performance capture. From multi-view video, they capture dynamic surface and texture models of the real world. Performance capture is hardly used in practice due to profound limitations: recording is usually limited to indoor studios, controlled lighting, and dense static camera arrays. Methods are often limited to single objects, and reconstructed shape detail is very limited. Assumptions about materials, reflectance, and lighting in a scene are simplistic, and we cannot easily modify captured data. In this project, we will pioneer a new generation of performance capture techniques to overcome these limitations. Our methods will allow the reconstruction of dynamic surface models of unprecedented shape detail. They will succeed on general scenes outside of the lab and outdoors, scenes with complex material and reflectance distributions, and scenes in which lighting is general, uncontrolled, and unknown. They will capture dense and crowded scenes with complex shape deformations. They will reconstruct conveniently modifiable scene models. They will work with sparse and moving sets of cameras, ultimately even with mobile phones. This far-reaching, multi-disciplinary project will turn performance capture from a research technology into a practical technology, provide groundbreaking scientific insights, and open up revolutionary new applications.",
    "origin": "Human"
}