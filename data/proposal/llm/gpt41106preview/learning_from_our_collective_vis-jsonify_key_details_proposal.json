{
    "item_type": "proposal",
    "title": "LEarning from our collective visual memory to Analyze its trends and Predict future events",
    "descriptions": [
        {
            "proposal_name": "LEarning from our collective visual memory to Analyze its trends and Predict future events",
            "proposal_details": {
                "descriptions": [
                    "People use past visual experiences to anticipate future events."
                ],
                "objectives": [
                    "Analyze dynamic patterns in shared visual experience",
                    "Find and quantify trends in visual data",
                    "Learn to predict future events in dynamic scenes"
                ],
                "challenges": [
                    "Invent new models for large-scale, spatio-temporal, distributed, and dynamic visual data",
                    "Generalize models across different data sources and imaging conditions",
                    "Measure long-term changes in related scenes",
                    "Synthesize complex future predictions from past visual experiences"
                ],
                "potential_impacts": [
                    "Safer cars with pedestrian behavior anticipation",
                    "Medical tools for monitoring, diagnosing, and predicting patients' health",
                    "Smart glasses aiding reaction in unfamiliar situations"
                ],
                "data_sources": [
                    "Public archives",
                    "Internet",
                    "Surveillance cameras",
                    "Personal cameras"
                ]
            }
        },
        {
            "proposal_name": "LEarning from our collective visual memory to Analyze its trends and Predict future events",
            "proposal_details": {
                "descriptions": [
                    "LEAP is a first step in analyzing and predicting events based on the collective visual memory formed by the vast amount of visual data available in public archives, on the Internet, and from surveillance or personal cameras."
                ],
                "goals": [
                    "Analyze dynamic patterns in shared visual experiences",
                    "Find and quantify trends in these experiences",
                    "Learn to predict future events in dynamic scenes"
                ],
                "challenges": [
                    "Inventing new and powerful models adapted to the scale and spatio-temporal, distributed and dynamic nature of the data"
                ],
                "methodologies": [
                    "Design new models that generalize across different data sources under varying imaging conditions",
                    "Develop a framework for finding, describing and quantifying trends",
                    "Develop a methodology and tools for synthesizing complex future predictions from past visual experiences"
                ],
                "potential_implications": [
                    "Safer cars that anticipate pedestrian behavior",
                    "Tools that assist doctors in monitoring, diagnosing, and predicting patients’ health",
                    "Smart glasses that help people react in unfamiliar situations"
                ]
            }
        },
        {
            "proposal_name": "LEarning from our collective visual memory to Analyze its trends and Predict future events",
            "proposal_details": {
                "descriptions": [
                    "LEAP aims to analyze and predict dynamic patterns in collective visual memory utilizing public archives, internet data, and camera footage."
                ],
                "goals": [
                    "Analyze dynamic patterns in shared visual experiences",
                    "Find and quantify trends in visual data",
                    "Learn to predict future events in dynamic scenes"
                ],
                "challenges": [
                    "Inventing new models for large-scale spatio-temporal data",
                    "Generalizing across different data sources",
                    "Measuring long-term changes in related scenes",
                    "Synthesizing predictions from past visual experiences"
                ],
                "approach": [
                    "Design new models for different imaging conditions",
                    "Develop a framework for finding and quantifying trends",
                    "Create a methodology for complex future predictions"
                ],
                "implications": [
                    "Safer cars anticipating pedestrian behavior",
                    "Tools aiding doctors in patient health monitoring and prediction",
                    "Smart glasses assisting people in unfamiliar situations"
                ]
            }
        }
    ],
    "origin": "LLM",
    "llm_engine": "gpt-4-1106-preview",
    "generation_prompt_uid": "7ee16c0406d1d081daa85eed7a306719",
    "generation_prompt_nickname": "jsonify_key_details_proposal",
    "generation_prompt_text": "Extract and present the key details from this grant proposal abstract in valid JSON format. Keep array structures simple and flat where possible. Focus only on capturing the concrete features, characteristics, and data points - exclude any narrative text or prose descriptions. The response should contain exactly one item in the 'descriptions' array!\n\n---\n\n**Title:**\n\nLEarning from our collective visual memory to Analyze its trends and Predict future events\n\n**Description:**\n\nPeople constantly draw on past visual experiences to anticipate future events and better understand, navigate, and interact with their environment, for example, when seeing an angry dog or a quickly approaching car. Currently there is no artificial system with a similar level of visual analysis and prediction capabilities. LEAP is a first step in that direction, leveraging the emerging collective visual memory formed by the unprecedented amount of visual data available in public archives, on the Internet and from surveillance or personal cameras - a complex evolving net of dynamic scenes, distributed across many different data sources, and equipped with plentiful but noisy and incomplete metadata. The goal of this project is to analyze dynamic patterns in this shared visual experience in order (i) to find and quantify their trends; and (ii) learn to predict future events in dynamic scenes. With ever expanding computational resources and this extraordinary data, the main scientific challenge is now to invent new and powerful models adapted to its scale and its spatio-temporal, distributed and dynamic nature. To address this challenge, we will first design new models that generalize across different data sources, where scenes are captured under vastly different imaging conditions. Next, we will develop a framework for finding, describing and quantifying trends that involve measuring long-term changes in many related scenes. Finally, we will develop a methodology and tools for synthesizing complex future predictions from aligned past visual experiences. Breakthrough progress on these problems would have profound implications on our everyday lives as well as science and commerce, with safer cars that anticipate the behavior of pedestrians on streets; tools that help doctors monitor, diagnose and predict patients’ health; and smart glasses that help people react in unfamiliar situations enabled by the advances from this project."
}